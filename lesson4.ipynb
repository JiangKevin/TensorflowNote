{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉商loss 函數 (cross entropy loss function)\n",
    "對於 softmax 激活函數的交叉商如下:\n",
    "$$J(\\theta ) = - \\frac{1} {m} \\sum_{i=1}^{m} y^{(i)} \\log(h_{\\theta} (x^{(i)})) + (1 - y^{(i)}) \\log (1 - h_{\\theta} (x^{(i)})) $$\n",
    "對於 softmax 等函數(S型曲線函數)，使用交叉商loss函數，可以收斂得更快!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter=0, Testing Accuracy=0.2917\n",
      "Iter=20, Testing Accuracy=0.7441\n",
      "Iter=40, Testing Accuracy=0.8007\n",
      "Iter=60, Testing Accuracy=0.8201\n",
      "Iter=80, Testing Accuracy=0.8202\n",
      "Iter=100, Testing Accuracy=0.8252\n",
      "Iter=120, Testing Accuracy=0.8229\n",
      "Iter=140, Testing Accuracy=0.8269\n",
      "Iter=160, Testing Accuracy=0.8256\n",
      "Iter=180, Testing Accuracy=0.8267\n",
      "Iter=200, Testing Accuracy=0.8334\n",
      "Iter=220, Testing Accuracy=0.9005\n",
      "Iter=240, Testing Accuracy=0.9043\n",
      "Iter=260, Testing Accuracy=0.9075\n",
      "Iter=280, Testing Accuracy=0.9085\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#載入數據集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) \n",
    "\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size \n",
    "\n",
    "#定義兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "\n",
    "#建立一個神經網路\n",
    "#隱藏層\n",
    "W1 = tf.Variable(tf.random_normal([784, 15]))\n",
    "b1 = tf.Variable(tf.zeros([1, 15]))\n",
    "L1 = tf.nn.softmax(tf.matmul(x, W1) + b1) #隱藏層的輸出\n",
    "\n",
    "#輸出層\n",
    "W = tf.Variable(tf.zeros([15, 10]))\n",
    "b = tf.Variable(tf.zeros([1, 10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(L1, W) + b)\n",
    "\n",
    "#代價函數 : loss = mean((y - prediction)^2)\n",
    "#loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))\n",
    "\n",
    "#Gradient desent method \n",
    "gd = tf.train.AdagradOptimizer(0.31)\n",
    "#gd = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化 代價函數 (operator) \n",
    "train = gd.minimize(loss)\n",
    "\n",
    "#初始化變數 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "#結果存在一個 boolean 的變數中\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 回傳一維張量中最大的值，所在的位置\n",
    "\n",
    "#求準確率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(300): \n",
    "       \n",
    "        for batch in range(n_batch): #每一個 outer loop 疊代 n_batch 個批次\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: batch_xs, y: batch_ys} \n",
    "            sess.run(train, feed_dict)\n",
    "        if epoch % 20 == 0:\n",
    "            #計算一次準確率\n",
    "            outer_loop_feed_dict = {x: mnist.test.images, y: mnist.test.labels} #testing data feed dictionary\n",
    "            acc = sess.run(accuracy, outer_loop_feed_dict)\n",
    "            print(\"Iter=\" + str(epoch) + \", Testing Accuracy=\" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dropout\n",
    "在訓練神經網路的時候，對於不一樣的訓練樣本，遮蔽隱藏層的一些神經元，可以減低 overfitting 的可能  \n",
    "以下是一個沒有 Dropout的例子 (keep_prob = 1.0)， Training Accuracy 比 Test Accuracy 準確許多  \n",
    "也就是說，這個神經網路已經 Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter=0, Training Accuracy=0.9368182, Testing Accuracy=0.9363\n",
      "Iter=1, Training Accuracy=0.9541636, Testing Accuracy=0.9508\n",
      "Iter=2, Training Accuracy=0.95910907, Testing Accuracy=0.9537\n",
      "Iter=3, Training Accuracy=0.96532726, Testing Accuracy=0.9597\n",
      "Iter=4, Training Accuracy=0.9710182, Testing Accuracy=0.9639\n",
      "Iter=5, Training Accuracy=0.97187275, Testing Accuracy=0.9632\n",
      "Iter=6, Training Accuracy=0.9745455, Testing Accuracy=0.9657\n",
      "Iter=7, Training Accuracy=0.9771091, Testing Accuracy=0.966\n",
      "Iter=8, Training Accuracy=0.9746364, Testing Accuracy=0.9658\n",
      "Iter=9, Training Accuracy=0.9768909, Testing Accuracy=0.9669\n",
      "Iter=10, Training Accuracy=0.9780909, Testing Accuracy=0.9644\n",
      "Iter=11, Training Accuracy=0.9803636, Testing Accuracy=0.9699\n",
      "Iter=12, Training Accuracy=0.98392725, Testing Accuracy=0.9737\n",
      "Iter=13, Training Accuracy=0.9858909, Testing Accuracy=0.974\n",
      "Iter=14, Training Accuracy=0.98765457, Testing Accuracy=0.9733\n",
      "Iter=15, Training Accuracy=0.9876909, Testing Accuracy=0.9747\n",
      "Iter=16, Training Accuracy=0.9883818, Testing Accuracy=0.9748\n",
      "Iter=17, Training Accuracy=0.98530906, Testing Accuracy=0.9684\n",
      "Iter=18, Training Accuracy=0.9884727, Testing Accuracy=0.9748\n",
      "Iter=19, Training Accuracy=0.9896909, Testing Accuracy=0.9741\n",
      "Iter=20, Training Accuracy=0.9897091, Testing Accuracy=0.9717\n",
      "Iter=21, Training Accuracy=0.9916546, Testing Accuracy=0.9759\n",
      "Iter=22, Training Accuracy=0.9920727, Testing Accuracy=0.976\n",
      "Iter=23, Training Accuracy=0.99194545, Testing Accuracy=0.9769\n",
      "Iter=24, Training Accuracy=0.9923091, Testing Accuracy=0.9747\n",
      "Iter=25, Training Accuracy=0.99334544, Testing Accuracy=0.9773\n",
      "Iter=26, Training Accuracy=0.99349093, Testing Accuracy=0.9777\n",
      "Iter=27, Training Accuracy=0.9939455, Testing Accuracy=0.9775\n",
      "Iter=28, Training Accuracy=0.99305457, Testing Accuracy=0.9772\n",
      "Iter=29, Training Accuracy=0.9948909, Testing Accuracy=0.9787\n",
      "Iter=30, Training Accuracy=0.9944909, Testing Accuracy=0.9775\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#載入數據集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) \n",
    "\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size \n",
    "\n",
    "#定義兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "keep_prob = tf.placeholder(tf.float32) #用來 dropout 的機率\n",
    "\n",
    "#建立一個神經網路\n",
    "\n",
    "#隱藏層1\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 2000], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([2000]))\n",
    "L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)\n",
    "L1_dropout = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "\n",
    "#隱藏層2\n",
    "W2 = tf.Variable(tf.truncated_normal([2000, 2000], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([2000]))\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_dropout, W2) + b2)\n",
    "L2_dropout = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "#隱藏層3\n",
    "W3 = tf.Variable(tf.truncated_normal([2000, 1000], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([1000]))\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_dropout, W3) + b3)\n",
    "L3_dropout = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "#輸出層\n",
    "W4 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.tanh(tf.matmul(L3_dropout, W4) + b4)\n",
    "\n",
    "\n",
    "#代價函數 : loss = mean((y - prediction)^2)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))\n",
    "\n",
    "#Gradient desent method \n",
    "gd = tf.train.AdagradOptimizer(0.31)\n",
    "#gd = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化 代價函數 (operator)\n",
    "train = gd.minimize(loss)\n",
    "\n",
    "#初始化變數 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "#結果存在一個 boolean 的變數中\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 回傳一維張量中最大的值，所在的位置\n",
    "\n",
    "#求準確率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(31): \n",
    "       \n",
    "        for batch in range(n_batch): #每一個 outer loop 疊代 n_batch 個批次\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: batch_xs, y: batch_ys, keep_prob: 1.0} \n",
    "            sess.run(train, feed_dict)\n",
    "        #計算一次準確率\n",
    "        train_feed_dict = {x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0} #train data feed dictionary\n",
    "        train_acc = sess.run(accuracy, train_feed_dict)\n",
    "        test_feed_dict = {x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0} #testing data feed dictionary\n",
    "        test_acc = sess.run(accuracy, test_feed_dict)          \n",
    "        print(\"Iter=\" + str(epoch) + \", Training Accuracy=\" + str(train_acc) + \", Testing Accuracy=\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定 keep_prob = 0.7，採用Dropout 的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter=0, Training Accuracy=0.8870182, Testing Accuracy=0.8944\n",
      "Iter=1, Training Accuracy=0.9125636, Testing Accuracy=0.9133\n",
      "Iter=2, Training Accuracy=0.8894, Testing Accuracy=0.895\n",
      "Iter=3, Training Accuracy=0.9266, Testing Accuracy=0.929\n",
      "Iter=4, Training Accuracy=0.9251091, Testing Accuracy=0.9259\n",
      "Iter=5, Training Accuracy=0.9354182, Testing Accuracy=0.9351\n",
      "Iter=6, Training Accuracy=0.9441091, Testing Accuracy=0.9429\n",
      "Iter=7, Training Accuracy=0.94243634, Testing Accuracy=0.9425\n",
      "Iter=8, Training Accuracy=0.9476, Testing Accuracy=0.9486\n",
      "Iter=9, Training Accuracy=0.9486727, Testing Accuracy=0.9445\n",
      "Iter=10, Training Accuracy=0.95247275, Testing Accuracy=0.9496\n",
      "Iter=11, Training Accuracy=0.9529091, Testing Accuracy=0.9506\n",
      "Iter=12, Training Accuracy=0.9524182, Testing Accuracy=0.9504\n",
      "Iter=13, Training Accuracy=0.9569091, Testing Accuracy=0.9552\n",
      "Iter=14, Training Accuracy=0.9564, Testing Accuracy=0.9524\n",
      "Iter=15, Training Accuracy=0.95854545, Testing Accuracy=0.9568\n",
      "Iter=16, Training Accuracy=0.95632726, Testing Accuracy=0.9537\n",
      "Iter=17, Training Accuracy=0.95843637, Testing Accuracy=0.9526\n",
      "Iter=18, Training Accuracy=0.9599818, Testing Accuracy=0.958\n",
      "Iter=19, Training Accuracy=0.96058184, Testing Accuracy=0.957\n",
      "Iter=20, Training Accuracy=0.9634182, Testing Accuracy=0.959\n",
      "Iter=21, Training Accuracy=0.9618545, Testing Accuracy=0.9569\n",
      "Iter=22, Training Accuracy=0.96238184, Testing Accuracy=0.9593\n",
      "Iter=23, Training Accuracy=0.96196365, Testing Accuracy=0.9563\n",
      "Iter=24, Training Accuracy=0.9616182, Testing Accuracy=0.9575\n",
      "Iter=25, Training Accuracy=0.9642, Testing Accuracy=0.9601\n",
      "Iter=26, Training Accuracy=0.9668546, Testing Accuracy=0.9609\n",
      "Iter=27, Training Accuracy=0.96654546, Testing Accuracy=0.9639\n",
      "Iter=28, Training Accuracy=0.9670182, Testing Accuracy=0.9631\n",
      "Iter=29, Training Accuracy=0.9666909, Testing Accuracy=0.962\n",
      "Iter=30, Training Accuracy=0.9669091, Testing Accuracy=0.9626\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#載入數據集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) \n",
    "\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size \n",
    "\n",
    "#定義兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "keep_prob = tf.placeholder(tf.float32) #用來 dropout 的機率\n",
    "\n",
    "#建立一個神經網路\n",
    "\n",
    "#隱藏層1\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 2000], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([2000]))\n",
    "L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)\n",
    "L1_dropout = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "\n",
    "#隱藏層2\n",
    "W2 = tf.Variable(tf.truncated_normal([2000, 2000], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([2000]))\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_dropout, W2) + b2)\n",
    "L2_dropout = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "#隱藏層3\n",
    "W3 = tf.Variable(tf.truncated_normal([2000, 1000], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([1000]))\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_dropout, W3) + b3)\n",
    "L3_dropout = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "#輸出層\n",
    "W4 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.tanh(tf.matmul(L3_dropout, W4) + b4)\n",
    "\n",
    "\n",
    "#代價函數 : loss = mean((y - prediction)^2)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))\n",
    "\n",
    "#Gradient desent method \n",
    "gd = tf.train.AdagradOptimizer(0.31)\n",
    "#gd = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化 代價函數 (operator)\n",
    "train = gd.minimize(loss)\n",
    "\n",
    "#初始化變數 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "#結果存在一個 boolean 的變數中\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 回傳一維張量中最大的值，所在的位置\n",
    "\n",
    "#求準確率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(31): \n",
    "       \n",
    "        for batch in range(n_batch): #每一個 outer loop 疊代 n_batch 個批次\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: batch_xs, y: batch_ys, keep_prob: 0.7} \n",
    "            sess.run(train, feed_dict)\n",
    "        #計算一次準確率\n",
    "        train_feed_dict = {x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0} #train data feed dictionary\n",
    "        train_acc = sess.run(accuracy, train_feed_dict)\n",
    "        test_feed_dict = {x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0} #testing data feed dictionary\n",
    "        test_acc = sess.run(accuracy, test_feed_dict)          \n",
    "        print(\"Iter=\" + str(epoch) + \", Training Accuracy=\" + str(train_acc) + \", Testing Accuracy=\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業  \n",
    "利用這周學到的技巧，讓MINIST 網路的 Test Accuracy 拿到 98% 以上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter=0, Training Accuracy=0.92867273, Testing Accuracy=0.9303\n",
      "Iter=1, Training Accuracy=0.94305456, Testing Accuracy=0.9415\n",
      "Iter=2, Training Accuracy=0.95403636, Testing Accuracy=0.9496\n",
      "Iter=3, Training Accuracy=0.96285456, Testing Accuracy=0.9586\n",
      "Iter=4, Training Accuracy=0.96805453, Testing Accuracy=0.9621\n",
      "Iter=5, Training Accuracy=0.9748, Testing Accuracy=0.9678\n",
      "Iter=6, Training Accuracy=0.9777091, Testing Accuracy=0.9683\n",
      "Iter=7, Training Accuracy=0.9809273, Testing Accuracy=0.9725\n",
      "Iter=8, Training Accuracy=0.98234546, Testing Accuracy=0.9716\n",
      "Iter=9, Training Accuracy=0.98261815, Testing Accuracy=0.9737\n",
      "Iter=10, Training Accuracy=0.98554546, Testing Accuracy=0.9749\n",
      "Iter=11, Training Accuracy=0.9852727, Testing Accuracy=0.9749\n",
      "Iter=12, Training Accuracy=0.98783636, Testing Accuracy=0.9768\n",
      "Iter=13, Training Accuracy=0.9865818, Testing Accuracy=0.9728\n",
      "Iter=14, Training Accuracy=0.98963636, Testing Accuracy=0.9763\n",
      "Iter=15, Training Accuracy=0.9901091, Testing Accuracy=0.9785\n",
      "Iter=16, Training Accuracy=0.99061817, Testing Accuracy=0.9785\n",
      "Iter=17, Training Accuracy=0.99047273, Testing Accuracy=0.977\n",
      "Iter=18, Training Accuracy=0.9915636, Testing Accuracy=0.9789\n",
      "Iter=19, Training Accuracy=0.9919091, Testing Accuracy=0.9802\n",
      "Iter=20, Training Accuracy=0.99192727, Testing Accuracy=0.9797\n",
      "Iter=21, Training Accuracy=0.99227273, Testing Accuracy=0.9802\n",
      "Iter=22, Training Accuracy=0.9924909, Testing Accuracy=0.9809\n",
      "Iter=23, Training Accuracy=0.99265456, Testing Accuracy=0.9797\n",
      "Iter=24, Training Accuracy=0.9928182, Testing Accuracy=0.9805\n",
      "Iter=25, Training Accuracy=0.9930909, Testing Accuracy=0.9803\n",
      "Iter=26, Training Accuracy=0.99314547, Testing Accuracy=0.9799\n",
      "Iter=27, Training Accuracy=0.9932, Testing Accuracy=0.9805\n",
      "Iter=28, Training Accuracy=0.99334544, Testing Accuracy=0.9811\n",
      "Iter=29, Training Accuracy=0.9935273, Testing Accuracy=0.9812\n",
      "Iter=30, Training Accuracy=0.9935455, Testing Accuracy=0.9815\n"
     ]
    }
   ],
   "source": [
    "#自己測出來的，與Ben 老師給的解法不同\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#載入數據集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n",
    "\n",
    "#每一個批次的大小\n",
    "batch_size = 80 \n",
    "\n",
    "#計算一共有多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size \n",
    "\n",
    "#定義兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "\n",
    "#建立一個神經網路\n",
    "\n",
    "#隱藏層\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 800], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([800]))\n",
    "L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)\n",
    "\n",
    "#輸出層\n",
    "W2 = tf.Variable(tf.truncated_normal([800, 10], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.tanh(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "#代價函數 : loss = mean((y - prediction)^2)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction)) #交叉商代價函數\n",
    "\n",
    "#Gradient desent method \n",
    "gd = tf.train.AdagradOptimizer(0.2)\n",
    "#gd = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化 代價函數 (operator)\n",
    "train = gd.minimize(loss)\n",
    "\n",
    "#初始化變數 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "#結果存在一個 boolean 的變數中\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 回傳一維張量中最大的值，所在的位置\n",
    "\n",
    "#求準確率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(31): \n",
    "       \n",
    "        for batch in range(n_batch): #每一個 outer loop 疊代 n_batch 個批次\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: batch_xs, y: batch_ys} \n",
    "            sess.run(train, feed_dict)\n",
    "        #計算一次準確率\n",
    "        train_feed_dict = {x: mnist.train.images, y: mnist.train.labels} #train data feed dictionary\n",
    "        train_acc = sess.run(accuracy, train_feed_dict)\n",
    "        test_feed_dict = {x: mnist.test.images, y: mnist.test.labels} #testing data feed dictionary\n",
    "        test_acc = sess.run(accuracy, test_feed_dict)          \n",
    "        print(\"Iter=\" + str(epoch) + \", Training Accuracy=\" + str(train_acc) + \", Testing Accuracy=\" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
