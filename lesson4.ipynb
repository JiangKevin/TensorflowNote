{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉商loss 函數 (cross entropy loss function)\n",
    "對於 softmax 激活函數的交叉商如下:\n",
    "$$J(\\theta ) = - \\frac{1} {m} \\sum_{i=1}^{m} y^{(i)} \\log(h_{\\theta} (x^{(i)})) + (1 - y^{(i)}) \\log (1 - h_{\\theta} (x^{(i)})) $$\n",
    "對於 softmax 等函數，使用交叉商loss函數，可以收斂得更快!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter=0, Testing Accuracy=0.2364\n",
      "Iter=20, Testing Accuracy=0.5778\n",
      "Iter=40, Testing Accuracy=0.5847\n",
      "Iter=60, Testing Accuracy=0.6285\n",
      "Iter=80, Testing Accuracy=0.6565\n",
      "Iter=100, Testing Accuracy=0.6605\n",
      "Iter=120, Testing Accuracy=0.8303\n",
      "Iter=140, Testing Accuracy=0.8963\n",
      "Iter=160, Testing Accuracy=0.9062\n",
      "Iter=180, Testing Accuracy=0.9083\n",
      "Iter=200, Testing Accuracy=0.9077\n",
      "Iter=220, Testing Accuracy=0.9099\n",
      "Iter=240, Testing Accuracy=0.9115\n",
      "Iter=260, Testing Accuracy=0.9106\n",
      "Iter=280, Testing Accuracy=0.9091\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#載入數據集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) \n",
    "\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size \n",
    "\n",
    "#定義兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "\n",
    "#建立一個神經網路\n",
    "#隱藏層\n",
    "W1 = tf.Variable(tf.random_normal([784, 15]))\n",
    "b1 = tf.Variable(tf.zeros([1, 15]))\n",
    "L1 = tf.nn.softmax(tf.matmul(x, W1) + b1) #隱藏層的輸出\n",
    "\n",
    "#輸出層\n",
    "W = tf.Variable(tf.zeros([15, 10]))\n",
    "b = tf.Variable(tf.zeros([1, 10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(L1, W) + b)\n",
    "\n",
    "#代價函數 : loss = mean((y - prediction)^2)\n",
    "#loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))\n",
    "\n",
    "#Gradient desent method \n",
    "gd = tf.train.AdagradOptimizer(0.31)\n",
    "#gd = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化 代價函數 (operator) \n",
    "train = gd.minimize(loss)\n",
    "\n",
    "#初始化變數 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "#結果存在一個 boolean 的變數中\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 回傳一維張量中最大的值，所在的位置\n",
    "\n",
    "#求準確率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(300): \n",
    "       \n",
    "        for batch in range(n_batch): #每一個 outer loop 疊代 n_batch 個批次\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: batch_xs, y: batch_ys} \n",
    "            sess.run(train, feed_dict)\n",
    "        if epoch % 20 == 0:\n",
    "            #計算一次準確率\n",
    "            outer_loop_feed_dict = {x: mnist.test.images, y: mnist.test.labels} #testing data feed dictionary\n",
    "            acc = sess.run(accuracy, outer_loop_feed_dict)\n",
    "            print(\"Iter=\" + str(epoch) + \", Testing Accuracy=\" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dropout\n",
    "在訓練神經網路的時候，對於不一樣的訓練樣本，遮蔽隱藏層的一些神經元，可以減低 overfitting 的可能  \n",
    "以下是一個沒有 Dropout的例子 (keep_prob = 1.0)， Training Accuracy 比 Test Accuracy 準確許多  \n",
    "也就是說，這個神經網路已經 Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter=0, Training Accuracy=0.9175091, Testing Accuracy=0.9168\n",
      "Iter=1, Training Accuracy=0.92858183, Testing Accuracy=0.9251\n",
      "Iter=2, Training Accuracy=0.96032727, Testing Accuracy=0.9585\n",
      "Iter=3, Training Accuracy=0.96132725, Testing Accuracy=0.9571\n",
      "Iter=4, Training Accuracy=0.9664, Testing Accuracy=0.9601\n",
      "Iter=5, Training Accuracy=0.9720727, Testing Accuracy=0.9643\n",
      "Iter=6, Training Accuracy=0.9748727, Testing Accuracy=0.9661\n",
      "Iter=7, Training Accuracy=0.9772364, Testing Accuracy=0.9669\n",
      "Iter=8, Training Accuracy=0.98005456, Testing Accuracy=0.9714\n",
      "Iter=9, Training Accuracy=0.976, Testing Accuracy=0.9651\n",
      "Iter=10, Training Accuracy=0.98081815, Testing Accuracy=0.9687\n",
      "Iter=11, Training Accuracy=0.98210907, Testing Accuracy=0.9723\n",
      "Iter=12, Training Accuracy=0.9780909, Testing Accuracy=0.967\n",
      "Iter=13, Training Accuracy=0.98301816, Testing Accuracy=0.9698\n",
      "Iter=14, Training Accuracy=0.9844, Testing Accuracy=0.9702\n",
      "Iter=15, Training Accuracy=0.9846727, Testing Accuracy=0.9718\n",
      "Iter=16, Training Accuracy=0.98783636, Testing Accuracy=0.975\n",
      "Iter=17, Training Accuracy=0.98843634, Testing Accuracy=0.974\n",
      "Iter=18, Training Accuracy=0.9891091, Testing Accuracy=0.9712\n",
      "Iter=19, Training Accuracy=0.9911818, Testing Accuracy=0.9765\n",
      "Iter=20, Training Accuracy=0.9901818, Testing Accuracy=0.9753\n",
      "Iter=21, Training Accuracy=0.9897636, Testing Accuracy=0.9736\n",
      "Iter=22, Training Accuracy=0.99194545, Testing Accuracy=0.9778\n",
      "Iter=23, Training Accuracy=0.9921091, Testing Accuracy=0.9776\n",
      "Iter=24, Training Accuracy=0.9929818, Testing Accuracy=0.9767\n",
      "Iter=25, Training Accuracy=0.9927273, Testing Accuracy=0.9774\n",
      "Iter=26, Training Accuracy=0.99314547, Testing Accuracy=0.9763\n",
      "Iter=27, Training Accuracy=0.99258184, Testing Accuracy=0.9766\n",
      "Iter=28, Training Accuracy=0.9935091, Testing Accuracy=0.976\n",
      "Iter=29, Training Accuracy=0.9942545, Testing Accuracy=0.9773\n",
      "Iter=30, Training Accuracy=0.99407274, Testing Accuracy=0.9775\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#載入數據集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) \n",
    "\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size \n",
    "\n",
    "#定義兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "keep_prob = tf.placeholder(tf.float32) #用來 dropout 的機率\n",
    "\n",
    "#建立一個神經網路\n",
    "\n",
    "#隱藏層1\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 2000], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([2000]))\n",
    "L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)\n",
    "L1_dropout = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "\n",
    "#隱藏層2\n",
    "W2 = tf.Variable(tf.truncated_normal([2000, 2000], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([2000]))\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_dropout, W2) + b2)\n",
    "L2_dropout = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "#隱藏層3\n",
    "W3 = tf.Variable(tf.truncated_normal([2000, 1000], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([1000]))\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_dropout, W3) + b3)\n",
    "L3_dropout = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "#輸出層\n",
    "W4 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.tanh(tf.matmul(L3_dropout, W4) + b4)\n",
    "\n",
    "\n",
    "#代價函數 : loss = mean((y - prediction)^2)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))\n",
    "\n",
    "#Gradient desent method \n",
    "gd = tf.train.AdagradOptimizer(0.31)\n",
    "#gd = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化 代價函數 (operator)\n",
    "train = gd.minimize(loss)\n",
    "\n",
    "#初始化變數 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "#結果存在一個 boolean 的變數中\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 回傳一維張量中最大的值，所在的位置\n",
    "\n",
    "#求準確率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(31): \n",
    "       \n",
    "        for batch in range(n_batch): #每一個 outer loop 疊代 n_batch 個批次\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: batch_xs, y: batch_ys, keep_prob: 1.0} \n",
    "            sess.run(train, feed_dict)\n",
    "        #計算一次準確率\n",
    "        train_feed_dict = {x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0} #train data feed dictionary\n",
    "        train_acc = sess.run(accuracy, train_feed_dict)\n",
    "        test_feed_dict = {x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0} #testing data feed dictionary\n",
    "        test_acc = sess.run(accuracy, test_feed_dict)          \n",
    "        print(\"Iter=\" + str(epoch) + \", Training Accuracy=\" + str(train_acc) + \", Testing Accuracy=\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定 keep_prob = 0.7，採用Dropout 的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter=0, Training Accuracy=0.88272727, Testing Accuracy=0.8929\n",
      "Iter=1, Training Accuracy=0.90992725, Testing Accuracy=0.9157\n",
      "Iter=2, Training Accuracy=0.9202727, Testing Accuracy=0.9234\n",
      "Iter=3, Training Accuracy=0.9366909, Testing Accuracy=0.9367\n",
      "Iter=4, Training Accuracy=0.9338, Testing Accuracy=0.9346\n",
      "Iter=5, Training Accuracy=0.92669094, Testing Accuracy=0.9249\n",
      "Iter=6, Training Accuracy=0.93585455, Testing Accuracy=0.939\n",
      "Iter=7, Training Accuracy=0.9428545, Testing Accuracy=0.9399\n",
      "Iter=8, Training Accuracy=0.9483455, Testing Accuracy=0.9487\n",
      "Iter=9, Training Accuracy=0.9434182, Testing Accuracy=0.9439\n",
      "Iter=10, Training Accuracy=0.9503818, Testing Accuracy=0.9497\n",
      "Iter=11, Training Accuracy=0.95156366, Testing Accuracy=0.9476\n",
      "Iter=12, Training Accuracy=0.95167273, Testing Accuracy=0.9489\n",
      "Iter=13, Training Accuracy=0.94976366, Testing Accuracy=0.9491\n",
      "Iter=14, Training Accuracy=0.95863634, Testing Accuracy=0.9574\n",
      "Iter=15, Training Accuracy=0.95896363, Testing Accuracy=0.9549\n",
      "Iter=16, Training Accuracy=0.9569455, Testing Accuracy=0.9559\n",
      "Iter=17, Training Accuracy=0.9606182, Testing Accuracy=0.958\n",
      "Iter=18, Training Accuracy=0.9585091, Testing Accuracy=0.9528\n",
      "Iter=19, Training Accuracy=0.9610364, Testing Accuracy=0.9577\n",
      "Iter=20, Training Accuracy=0.9638182, Testing Accuracy=0.9609\n",
      "Iter=21, Training Accuracy=0.9631818, Testing Accuracy=0.9588\n",
      "Iter=22, Training Accuracy=0.96270907, Testing Accuracy=0.9591\n",
      "Iter=23, Training Accuracy=0.9649091, Testing Accuracy=0.9592\n",
      "Iter=24, Training Accuracy=0.9664182, Testing Accuracy=0.9637\n",
      "Iter=25, Training Accuracy=0.966, Testing Accuracy=0.9615\n",
      "Iter=26, Training Accuracy=0.9672, Testing Accuracy=0.9613\n",
      "Iter=27, Training Accuracy=0.9655273, Testing Accuracy=0.9586\n",
      "Iter=28, Training Accuracy=0.96867275, Testing Accuracy=0.9636\n",
      "Iter=29, Training Accuracy=0.96787274, Testing Accuracy=0.9628\n",
      "Iter=30, Training Accuracy=0.97007275, Testing Accuracy=0.9639\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#載入數據集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) \n",
    "\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size \n",
    "\n",
    "#定義兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "keep_prob = tf.placeholder(tf.float32) #用來 dropout 的機率\n",
    "\n",
    "#建立一個神經網路\n",
    "\n",
    "#隱藏層1\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 2000], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([2000]))\n",
    "L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)\n",
    "L1_dropout = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "\n",
    "#隱藏層2\n",
    "W2 = tf.Variable(tf.truncated_normal([2000, 2000], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([2000]))\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_dropout, W2) + b2)\n",
    "L2_dropout = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "#隱藏層3\n",
    "W3 = tf.Variable(tf.truncated_normal([2000, 1000], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([1000]))\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_dropout, W3) + b3)\n",
    "L3_dropout = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "#輸出層\n",
    "W4 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.tanh(tf.matmul(L3_dropout, W4) + b4)\n",
    "\n",
    "\n",
    "#代價函數 : loss = mean((y - prediction)^2)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))\n",
    "\n",
    "#Gradient desent method \n",
    "gd = tf.train.AdagradOptimizer(0.31)\n",
    "#gd = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化 代價函數 (operator)\n",
    "train = gd.minimize(loss)\n",
    "\n",
    "#初始化變數 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "#結果存在一個 boolean 的變數中\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 回傳一維張量中最大的值，所在的位置\n",
    "\n",
    "#求準確率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(31): \n",
    "       \n",
    "        for batch in range(n_batch): #每一個 outer loop 疊代 n_batch 個批次\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: batch_xs, y: batch_ys, keep_prob: 0.7} \n",
    "            sess.run(train, feed_dict)\n",
    "        #計算一次準確率\n",
    "        train_feed_dict = {x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0} #train data feed dictionary\n",
    "        train_acc = sess.run(accuracy, train_feed_dict)\n",
    "        test_feed_dict = {x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0} #testing data feed dictionary\n",
    "        test_acc = sess.run(accuracy, test_feed_dict)          \n",
    "        print(\"Iter=\" + str(epoch) + \", Training Accuracy=\" + str(train_acc) + \", Testing Accuracy=\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業  \n",
    "利用這周學到的技巧，讓MINIST 網路的 Test Accuracy 拿到 98% 以上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
